{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov5_use_custom_data1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1dL4ZbE2YRXG"
      ],
      "machine_shape": "hm",
      "mount_file_id": "124Bq39HP-d28Hdyx3QJmIeLIxWEoMskR",
      "authorship_tag": "ABX9TyNhN+ljrE/Ng6F7j4Fj7qa7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kikiru328/Bone_Detection/blob/main/%ED%98%9C%EC%A0%95/yolov5_use(%EC%A0%95%EB%A6%ACX).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dL4ZbE2YRXG"
      },
      "source": [
        "## 1. 사전학습모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lXuDqEAG-nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96d0350-ff2f-41a2-97a2-8dab171ca84c"
      },
      "source": [
        "# 코랩을 이용해서 사용하였습니다.\n",
        "# 아래의 과정은 yolov5 튜토리얼과정중 일부입니다.\n",
        "# 깃허브 링크: https://github.com/ultralytics/yolov5 , 튜토리얼 링크 : https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\n",
        "\n",
        "# 1) 먼저, yolo 설치가 필요합니다. ===============================================================\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.9.0+cu111 (Tesla V100-SXM2-16GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpkRtWzAHU7l"
      },
      "source": [
        "# train_data.zip 파일을 저는 '/content'(%pwd) 에 넣었습니다.\n",
        "# trani_data.zip 파일의 구성 : images 와 labels 폴더를 만들어 뒀습니다. 각 폴더 안에는 train 과 val 폴더가 있고, 사진과 라벨이 들어가있습니다.\n",
        "# 2) train_data upzip 해줍니다===============================================================\n",
        "# !unzip -q ../train_data.zip -d ../"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9JCSZWkIJ9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66d344e-24b1-40f0-c3d8-4eb10baa5ff8"
      },
      "source": [
        "# /content의 yolov5 폴더안의 data폴더에 custom_data.yaml 파일을 넣어줍니다.\n",
        "# custom_data의 내용을 다음과 같습니다.\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# train: /content/train_data/images/train  # train images (relative to 'path') 50 images\n",
        "# val: /content/train_data/images/val  # val images (relative to 'path') 10 images\n",
        "# test:  # test images (optional)\n",
        "\n",
        "# # Classes\n",
        "# nc: 7  # number of classes\n",
        "# names: ['CARPAL', 'LMCP', 'MMCP', 'TMCP', 'LPIP', 'MPIP', 'IP']  # class names\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# 3) *** batch, epochs 조절하고, custom_data.yaml 으로 수정해서 폴더에 넣어주고 실행합니다. ==============================================================\n",
        "!python train.py --img 800 --batch 150 --epochs 100 --data custom_data.yaml --weights yolov5s.pt --cache\n",
        "\n",
        "# 3번 과정의 실행이 완료되면 맨 마지막에 Results saved to runs/train/exp 라고 저장된 경로가 뜹니다.\n",
        "# yolov5 폴더에 경로를 따라 들어가시면 확인하실 수 있습니다.\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=custom_data.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=100, batch_size=150, imgsz=800, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.0-39-g5d4258f torch 1.9.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     32364  models.yolo.Detect                      [7, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7038508 parameters, 7038508 gradients, 15.9 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.001171875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/team2/preprocessing_com/train_data/labels/train.cache' images and labels... 560 found, 0 missing, 0 empty, 0 corrupted: 100% 560/560 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.8GB ram): 100% 560/560 [00:01<00:00, 493.31it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/team2/preprocessing_com/train_data/labels/val.cache' images and labels... 61 found, 0 missing, 0 empty, 0 corrupted: 100% 61/61 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 61/61 [00:00<00:00, 156.38it/s]\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.89, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 800 train, 800 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "  0% 0/4 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 627, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 524, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 297, in train\n",
            "    imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 1.07 GiB (GPU 0; 15.78 GiB total capacity; 1.13 GiB already allocated; 497.75 MiB free; 1.17 GiB reserved in total by PyTorch)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiOjsA1lJaZE"
      },
      "source": [
        "# +) 박스 예측, test폴더를 만들어서 지정해주셔도 되고, 파일만 넣어도 detect 됩니다. 튜토리얼 과정을 참고하여 주세요. 여기서 저는 이것을 활용하지 않았습니다. \n",
        "# !python detect.py --weights runs/train/exp/weights/last.pt --img 320 --conf 0.25 --source ../717.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LFd6QAaYYrS"
      },
      "source": [
        "## 2. 객체 인식 및 추출하기."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9t8xsFTJU9r"
      },
      "source": [
        "# 4) 사전학습된 모델을 가져와서 예측(객체인식 및 추출)합니다. ==============================================================\n",
        "\n",
        "#!pip install -qr requirements.txt 에 대한 런타임 재시작이 필요하다고 떠서 런타임을 재시작해 주었습니다.\n",
        "# torch.hub.load 는 깃허브 리포지토리 또는 로컬 디렉터리에서 모델을 로드합니다. # 자세한 설명은 설명 사이트(https://pytorch.org/docs/stable/hub.html)를 참고해 주세요. # https://runebook.dev/ko/docs/pytorch/hub 이 사이트에서도 설명이 잘 되어있습니다.\n",
        "# 리포지토리 또는 로컬 디렉터리 입력후 custom 이 들어간 자리는 모델명입니다. \n",
        "# model = torch.hub.load('repo_or_dir', 'model')  # yolov5에서 모델 종류는 yolov5s or yolov5m, yolov5l, yolov5x, custom 이렇게 있습니다.\n",
        "\n",
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/yolov5/runs/train/exp/weights/last.pt', force_reload=True) \n",
        "\n",
        "\n",
        "# custom 모델은 path에 파일(가중치파일) 경로를 적용하여 사전 훈련된 모델을 생성합니다. \n",
        "# 위(로컬)에서 train 과 val 데이터로 사전 훈련된 모델(즉, 모델 정의 및 사전 훈련 된 가중치)을 가져와서 사용하였습니다.\n",
        "\n",
        "# 샘플 이미지를 모델에 넣어보고 예측값을 받아옵니다.\n",
        "img = '/content/sample.jpg'  # 이미지 경로를 지정해줍니다.\n",
        "results = model(img)  \n",
        "# crops = results.crop(save=True)  - render() 전에 crop 사용하면 박스 쳐지기 전에 잘라서 저장됨!!!  < 저희가 모을 파일입니다ㅎ\n",
        "\n",
        "# results.print() 모델에 적용된 결과값이 출력됩니다.\n",
        "# image 1/1: 800x600 1 CARPAL, 1 LMCP, 1 MMCP, 1 TMCP, 1 LPIP, 1 MPIP, 1 IP\n",
        "# 이미지 하나를 적용했고, 크기는 800*600 이며 인식된 객체의 라벨과 개수입니다.\n",
        "# Speed: 12.7ms pre-process, 12.9ms inference, 1.4ms NMS per image at shape (1, 3, 640, 480) # 그 밖의 결과입니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA1DxOLMRN1Q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# results.imgs # array of original images (as np array) passed to model for inference\n",
        "# results.render()  # updates results.imgs with boxes and labels\n",
        "# render()을 이용하여 라벨이 붙은 이미지결과파일을 불러와서 np.squeeze로 길이가 1인 축을 제거합니다. - results의 image shape가 (1, 3, 640, 480) 인것을 위(results.print())에서 확인할 수 있었습니다.\n",
        "\n",
        "plt.imshow(np.squeeze(results.render()))\n",
        "crops = results.crop(save=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRbGB2KSVivT"
      },
      "source": [
        "# 박스쳐진부분 잘려서 저장됨. 경로설정 가능. \n",
        "# render()후라서 박스와 같이 저장됨.\n",
        "crops = results.crop(save=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L27n902-XmuD"
      },
      "source": [
        "# 박스쳐진 전체 이미지를 저장합니다.\n",
        "save_img = np.squeeze(results.render())\n",
        "import cv2\n",
        "cv2.imwrite('/content/sample.jpg', save_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTJyr8YzX8LP"
      },
      "source": [
        "# 결과를 pandas로 받아올 수 있습니다.\n",
        "print(results.pandas().xyxy[0])\n",
        "#          xmin        ymin       xmax      ymax  confidence  class    name\n",
        "# 0  171.875000  450.312500  390.00000  630.9375    0.937988      0  CARPAL\n",
        "# 1  230.625000  274.218750  287.50000  350.0000    0.911621      2    MMCP\n",
        "# 2  421.250000  377.187500  481.25000  429.6875    0.902832      3    TMCP\n",
        "# 3  220.468750  144.140625  269.53125  182.8125    0.895020      5    MPIP\n",
        "# 4   67.500000  253.125000  108.43750  291.5625    0.890137      4    LPIP\n",
        "# 5  124.765625  327.187500  185.62500  382.8125    0.873047      1    LMCP\n",
        "# 6  487.187500  309.843750  532.81250  352.8125    0.850586      6      IP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHK_irUy_B75"
      },
      "source": [
        "## 샘플"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZhSixfO-_Li"
      },
      "source": [
        "!pip install -qr requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHbvjBR7_KDT",
        "outputId": "95f5a1bb-f589-461c-a0dd-f84bf3815d8f"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/drive/MyDrive/team2/last.pt', force_reload=True) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2021-11-1 torch 1.9.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7029004 parameters, 0 gradients, 15.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1RwaV4fC02q",
        "outputId": "3ae206ad-1a94-4a00-b447-bd338459490e"
      },
      "source": [
        "# model1 = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/best_0.pt', force_reload=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2021-10-31 torch 1.9.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7029004 parameters, 0 gradients, 15.9 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLO2PubD_KJd",
        "outputId": "8d337dc2-3d1a-406d-f5ff-9f329d2c6553"
      },
      "source": [
        "# Images # 572번까지, 230제외\n",
        "imgs = []\n",
        "for i in range(1,230):\n",
        "  try :\n",
        "    img = f'/content/drive/MyDrive/team2/preprocessing_com/female/{i}_F.jpg'\n",
        "    imgs.append(img)\n",
        "  except:\n",
        "    print(i)\n",
        "    continue\n",
        "\n",
        "print(len(imgs))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBtnNOSHENlz",
        "outputId": "e517f47a-4edc-4e16-a40c-beedaeb15fec"
      },
      "source": [
        "for i in range(231,573):\n",
        "  try :\n",
        "    img = f'/content/drive/MyDrive/team2/preprocessing_com/female/{i}_F.jpg'\n",
        "    imgs.append(img)\n",
        "  except:\n",
        "    print(i)\n",
        "    continue\n",
        "\n",
        "print(len(imgs))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sCRKDcJ7pZX",
        "outputId": "60c28615-0e79-4bf7-fd98-4e11cfbdf840"
      },
      "source": [
        "results1 = model(imgs)\n",
        "results1.render()\n",
        "crops1 = results1.crop(save=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saved 571 images to \u001b[1mruns/detect/exp2\u001b[0m\n",
            "Saved results to runs/detect/exp2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W31-Uljb9njg"
      },
      "source": [
        "import cv2\n",
        "save_path = '/content/drive/MyDrive/team2/ppt/female/render/'\n",
        "for i in range(0,572):\n",
        "  plt_img = np.squeeze(results1.render())[i]\n",
        "  cv2.imwrite(save_path+f'{i+1}_F.jpg', plt_img)\n",
        "  # plt.imshow(plt_img)\n",
        "  # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uWToFr8HbBs"
      },
      "source": [
        "imgs = []\n",
        "for i in range(1,666):\n",
        "  try :\n",
        "    img = f'/content/drive/MyDrive/team2/preprocessing_com/male/{i}_M.jpg'\n",
        "    imgs.append(img)\n",
        "  except:\n",
        "    print(i)\n",
        "    continue\n",
        "\n",
        "print(len(imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P44AU2qjHdgD"
      },
      "source": [
        "results2 = model(imgs)\n",
        "crops3 = results2.crop(save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk_kgfSIHh-a"
      },
      "source": [
        "import cv2\n",
        "save_path = '/content/drive/MyDrive/team2/ppt/male/render/'\n",
        "for i in range(0,665):\n",
        "  plt_img = np.squeeze(results2.render())[i]\n",
        "  cv2.imwrite(save_path+f'{i+1}_M.jpg', plt_img)\n",
        "  # plt.imshow(plt_img)\n",
        "  # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr3bSFthH2MD"
      },
      "source": [
        "def save_img(name,mm):\n",
        "  for i in range(1,573):\n",
        "    try : \n",
        "      if i != 230 :\n",
        "        img = f'/content/runs/detect/exp2/crops/{name}/{i}_F.jpg'\n",
        "        img = cv2.imread(img)\n",
        "        save_img = f'/content/drive/MyDrive/team2/ppt/{name}/{i}_F_{mm}.jpg'\n",
        "        cv2.imwrite(save_img, img)\n",
        "      except:\n",
        "        print(name, i)\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BVRDfiIPuC"
      },
      "source": [
        "save_img('CARPAL','cp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehMiMvJsIRy6"
      },
      "source": [
        "save_img('IP','ip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xwfcjYTIR3z"
      },
      "source": [
        "save_img('LMCP','lm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNvICV_XIR8K"
      },
      "source": [
        "save_img('LPIP','lp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4LRYIAYISAx"
      },
      "source": [
        "save_img('MMCP','mm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igw4EuEBISEp"
      },
      "source": [
        "save_img('MPIP','mp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nIMh6LZISJL"
      },
      "source": [
        "save_img('TMCP','tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAFg0RSxIqwL"
      },
      "source": [
        "# import cv2\n",
        "def save_img_male(name,mm):\n",
        "  for i in range(1,666):\n",
        "    try : \n",
        "      img = f'/content/runs/detect/exp3/crops/{name}/{i}_M.jpg'\n",
        "      img = cv2.imread(img)\n",
        "      save_img = f'/content/drive/MyDrive/team2/ppt/{name}/{i}_M_{mm}.jpg'\n",
        "      cv2.imwrite(save_img, img)\n",
        "    except:\n",
        "      print(name, i)\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsohg3_6JWpy"
      },
      "source": [
        "save_img_male('CARPAL','cp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCIcQEcpJWp3"
      },
      "source": [
        "save_img_male('IP','ip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gMcIem7JWp3"
      },
      "source": [
        "save_img_male('LMCP','lm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD_Nb6KbJWp3"
      },
      "source": [
        "save_img_male('LPIP','lp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_uO14oJWp3"
      },
      "source": [
        "save_img_male('MMCP','mm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbLN45mPJWp4"
      },
      "source": [
        "save_img_male('MPIP','mp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbC01FwWJWp4"
      },
      "source": [
        "save_img_male('TMCP','tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFdPj5xgIq0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zkPM-00Iq26"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFkYmCCpH2Pb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuVu4Q-jH2Ry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSwaMKScH2Ty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "EX2_KSu67t1H",
        "outputId": "3f003002-787b-46c0-e3f7-dfd53eb5095f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.imshow(np.squeeze(results2.render()))\n",
        "crops = results2.crop(save=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-743366acee6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# crops1 = results1.crop(save=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (571, 800, 600, 3) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF25w3kvMUAH",
        "outputId": "74dbb378-7b91-4e08-a381-7276aa79bdbc"
      },
      "source": [
        "imgs = []\n",
        "for i in range(1,666):\n",
        "  try :\n",
        "    img = f'/content/drive/MyDrive/team2/preprocessing_com/male/{i}_M.jpg'\n",
        "    imgs.append(img)\n",
        "  except:\n",
        "    print(i)\n",
        "    continue\n",
        "\n",
        "print(len(imgs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roCF5xQt_KL8"
      },
      "source": [
        "# results1 = model(imgs)\n",
        "results2 = model1(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMkOIZkyBjl5",
        "outputId": "c6bfa95a-5adf-45a4-eb3e-b208409d06ed"
      },
      "source": [
        "crops2 = results2.crop(save=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saved 665 images to \u001b[1mruns/detect/exp4\u001b[0m\n",
            "Saved results to runs/detect/exp4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYSEwOjN71LX"
      },
      "source": [
        "plt.imshow(np.squeeze(results2.render()[0]))\n",
        "crops = results2.crop(save=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wnZ-bK8Fypg"
      },
      "source": [
        "import cv2\n",
        "for i in range(1,573):\n",
        "  if i != 230 :\n",
        "    img = f'/content/runs/detect/exp3/crops/CARPAL/{i}_F.jpg'\n",
        "    img = cv2.imread(img)\n",
        "    save_img = f'/content/drive/MyDrive/team2/ppt/CARPAL/{i}_F.jpg'\n",
        "    cv2.imwrite(save_img, img)\n",
        "  else : continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U00BNAcPIO_B"
      },
      "source": [
        "# import cv2\n",
        "def save_img(name,mm):\n",
        "  for i in range(1,573):\n",
        "    if i != 230 :\n",
        "      img = f'/content/runs/detect/exp3/crops/{name}/{i}_F.jpg'\n",
        "      img = cv2.imread(img)\n",
        "      save_img = f'/content/drive/MyDrive/team2/ppt/{name}/{i}_F_{mm}.jpg'\n",
        "      cv2.imwrite(save_img, img)\n",
        "    else : continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGJzgfKZI6s5"
      },
      "source": [
        "save_img('CARPAL','cp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCU8Bnw6beaZ"
      },
      "source": [
        "save_img('IP','ip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjHvHOlbgFw"
      },
      "source": [
        "save_img('LMCP','lm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkaM-SaHbg8f"
      },
      "source": [
        "save_img('LPIP','lp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-8u2mukbh1p"
      },
      "source": [
        "save_img('MMCP','mm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF9aS0b5biTq"
      },
      "source": [
        "save_img('MPIP','mp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qUkRtlEbjEp"
      },
      "source": [
        "save_img('TMCP','tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTC6G8avLEqH",
        "outputId": "1128b7d2-affb-4e73-a87b-f480f1f62c78"
      },
      "source": [
        "imgs = []\n",
        "for i in range(1,667):\n",
        "  try :\n",
        "    img = f'/content/drive/MyDrive/team2/preprocessing_com/male/{i}_M.jpg'\n",
        "    imgs.append(img)\n",
        "  except:\n",
        "    print(i)\n",
        "    continue\n",
        "\n",
        "print(len(imgs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSpIEtPUK5QU"
      },
      "source": [
        "# import cv2\n",
        "def save_img_male(name,mm):\n",
        "  for i in range(1,666):\n",
        "    try : \n",
        "      img = f'/content/runs/detect/exp4/crops/{name}/{i}_M.jpg'\n",
        "      img = cv2.imread(img)\n",
        "      save_img = f'/content/drive/MyDrive/team2/crop_img/male/{name}/{i}_M_{mm}.jpg'\n",
        "      cv2.imwrite(save_img, img)\n",
        "    except:\n",
        "      print(name, i)\n",
        "      continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGlQTP5PMDfr"
      },
      "source": [
        "save_img_male('CARPAL','cp')\n",
        "save_img_male('IP','ip')\n",
        "save_img_male('LMCP','lm')\n",
        "save_img_male('LPIP','lp') #537, 647  < last.pt로는 해결됨. \n",
        "save_img_male('MMCP','mm')\n",
        "save_img_male('MPIP','mp')\n",
        "save_img_male('TMCP','tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBZJoECFPu18",
        "outputId": "5a1af1bb-095a-43c5-84f0-2ad5a28bff9f"
      },
      "source": [
        "again_img = '/content/drive/MyDrive/team2/preprocessing_com/male/537_M.jpg'\n",
        "results2 = model1(again_img)\n",
        "crops2 = results2.crop(save=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saved 1 image to \u001b[1mruns/detect/exp5\u001b[0m\n",
            "Saved results to runs/detect/exp5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnslhRpdQJvp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.imshow(np.squeeze(results2.render()))\n",
        "crops = results2.crop(save=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzBDsvzWQ-hl",
        "outputId": "d253dd43-db4c-4821-9aa6-e1f5506dd14e"
      },
      "source": [
        "again_img = '/content/drive/MyDrive/team2/preprocessing_com/male/537_M.jpg'\n",
        "results1 = model(again_img)\n",
        "crops1 = results1.crop(save=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saved 1 image to \u001b[1mruns/detect/exp6\u001b[0m\n",
            "Saved results to runs/detect/exp6\n",
            "\n"
          ]
        }
      ]
    }
  ]
}